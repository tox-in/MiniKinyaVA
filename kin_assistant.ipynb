{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Install Basic Packages for KINYA TTS To Work\n",
    "# This command installs the Kinyarwanda Text-to-Speech (TTS) package from a local directory\n",
    "# in development mode to enable real-time code modifications without reinstallation\n",
    "!pip install -e /content/drive/MyDrive/Kin-Assistant/Inference/\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Monotonic_Align-dependency Issues handle"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Install specific version of NumPy to ensure compatibility with monotonic_align\n",
    "# This prevents version conflicts that can break the alignment module\n",
    "!pip install --force-reinstall \"numpy<2.1.0,>1.26.0\"\n",
    "\n",
    "# Install Cython which is required for compiling the monotonic_align C extension\n",
    "# The --force-install flag ensures any existing installation is overwritten\n",
    "!pip install --force-install Cython"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Install Cython and NumPy packages\n",
    "# Cython is needed for the C extensions compilation\n",
    "# NumPy provides the array operations required by the TTS model\n",
    "!pip install Cython numpy"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Change directory to the monotonic_align module location\n",
    "%cd /content/drive/MyDrive/Kin-Assistant/Inference/monotonic_align\n",
    "\n",
    "# Create the monotonic_align directory if it doesn't exist\n",
    "# This directory will contain the compiled C extension\n",
    "!mkdir -p monotonic_align\n",
    "\n",
    "# Compile the C extension using Cython\n",
    "# The --inplace flag builds the extension in the source directory\n",
    "# This is required for the duration alignment algorithm in the TTS system\n",
    "!python setup.py build_ext --inplace"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Change directory to the monotonic_align module\n",
    "%cd /content/drive/MyDrive/Kin-Assistant/Inference/monotonic_align\n",
    "\n",
    "# Install the monotonic_align package in development mode\n",
    "# This allows importing the module after compilation\n",
    "# The -e flag (editable mode) enables changes without reinstallation\n",
    "!pip install -e ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test if the monotonic_align module was properly installed and can be imported\n",
    "# This verification is crucial before proceeding with the TTS system\n",
    "import monotonic_align\n",
    "print(\"Successfully imported!\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# STT Setup\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Upgrade pip to the latest version to avoid compatibility issues\n",
    "# --no-cache-dir ensures a clean installation without using cached packages\n",
    "%pip install --no-cache-dir --upgrade pip\n",
    "\n",
    "# Install Cython which is required for the Speech-to-Text (STT) compilation\n",
    "# Cython allows Python code to achieve C-like performance\n",
    "%pip install --no-cache-dir cython\n",
    "\n",
    "# Install all STT dependencies from the requirements file\n",
    "# This includes packages necessary for the NVIDIA NeMo framework\n",
    "%pip install --no-cache-dir -r /content/drive/MyDrive/Kin-Assistant/stt/requirements.txt\n",
    "\n",
    "# Install system packages (commented out, uncomment if needed)\n",
    "# This would install system dependencies listed in packages.txt\n",
    "# %apt-get update && cat /content/drive/MyDrive/Kin-Assistant/stt/packages.txt | xargs apt-get install -y\n",
    "\n",
    "# Install a specific version of Google Protobuf\n",
    "# Version 4.21.1 is required for compatibility with the NeMo models\n",
    "%pip install protobuf==4.21.1"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Install system-level audio processing dependencies\n",
    "# sox: Sound eXchange, a command-line utility for audio processing\n",
    "# libsox-fmt-all: Additional format libraries for sox to handle various audio formats\n",
    "# These are essential for audio processing in the STT pipeline\n",
    "!apt-get update && apt-get install -y sox libsox-fmt-all"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Authenticate with Hugging Face Hub using the stored token\n",
    "# This allows downloading models from Hugging Face repositories\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Retrieve the Hugging Face token from Colab secrets\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "\n",
    "# Log in to Hugging Face Hub with the token\n",
    "# add_to_git_credential=True allows git operations with Hugging Face repositories\n",
    "login(token=hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Install Gradio - a Python library for creating UI components for ML models\n",
    "# Gradio will be used to create a web interface for the voice assistant\n",
    "# This enables easy interaction with the STT and TTS components\n",
    "!pip install gradio"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "### Handle numpy version compatibility issues\n",
    "# Installing a specific version of NumPy (1.24.3) that is known to work with the STT model\n",
    "# This prevents \"RuntimeError: Numpy is not available\" errors that can occur with certain versions\n",
    "!pip install numpy==1.24.3"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "### Main Function of my system.\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import gradio as gr\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torchaudio\n",
    "from typing import Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# -------------------- TTS Setup (Your Existing Code) --------------------\n",
    "from kinyatts.tts.commons import intersperse\n",
    "from kinyatts.tts.utils import get_hparams_from_file, load_checkpoint\n",
    "from kinyatts.tts.models import SynthesizerTrn\n",
    "from kinyatts.tts.text import text_to_sequence\n",
    "from kinyatts.tts.text.symbols import symbols\n",
    "\n",
    "import os\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from pydub import AudioSegment\n",
    "import pyaudioconvert as pac\n",
    "import numpy as np\n",
    "import time as time\n",
    "import uuid\n",
    "\n",
    "#import the model using hugging face\n",
    "hf_model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(model_name=\"mbazaNLP/Kinyarwanda_nemo_stt_conformer_model\")\n",
    "\n",
    "# TTS Global variables\n",
    "inference_engine = (None, None, None, None)\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(\"/content/sounds\", exist_ok=True)\n",
    "os.makedirs(\"/content/outputs\", exist_ok=True)\n",
    "\n",
    "def kinya_tts_setup():\n",
    "    global inference_engine\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "\n",
    "    path_to_tts_config = '/content/drive/MyDrive/Kin-Assistant/Inference/kinyatts/ms_ktjw_istft_vits2_base.json'\n",
    "    path_to_tts_model = '/content/drive/MyDrive/Kin-Assistant/TTS_MODEL_ms_ktjw_istft_vits2_base_1M.pt'\n",
    "    tts_hps = get_hparams_from_file(path_to_tts_config)\n",
    "\n",
    "    if \"use_mel_posterior_encoder\" in tts_hps.model.keys() and tts_hps.model.use_mel_posterior_encoder == True:\n",
    "        print(\"Using mel posterior encoder for VITS2\")\n",
    "        posterior_channels = 80  # vits2\n",
    "        tts_hps.data.use_mel_posterior_encoder = True\n",
    "    else:\n",
    "        print(\"Using lin posterior encoder for VITS1\")\n",
    "        posterior_channels = tts_hps.data.filter_length // 2 + 1\n",
    "        tts_hps.data.use_mel_posterior_encoder = False\n",
    "    tts_model = SynthesizerTrn(\n",
    "        len(symbols),\n",
    "        posterior_channels,\n",
    "        tts_hps.train.segment_size // tts_hps.data.hop_length,\n",
    "        n_speakers=tts_hps.data.n_speakers, #- >0 for multi speaker\n",
    "        **tts_hps.model).to(device)\n",
    "    _ = tts_model.eval()\n",
    "    _ = load_checkpoint(path_to_tts_model, tts_model, None)\n",
    "\n",
    "    louder_vol = torchaudio.transforms.Vol(gain=3.0, gain_type=\"amplitude\")\n",
    "\n",
    "    inference_engine = (device, tts_model, tts_hps, louder_vol)\n",
    "\n",
    "    print('TTS API engine ready!', flush=True)\n",
    "\n",
    "# -------------------- Transcriber Setup (Your Existing Code) --------------------\n",
    "class Transcriber:\n",
    "    def __init__(self, audio_bytes : bytes) -> None:\n",
    "        self.audio_bytes = audio_bytes\n",
    "\n",
    "        #save the audio\n",
    "        self.save_audio()\n",
    "\n",
    "        #convert the audio file\n",
    "        self.convert_wav_to_16bit_mono()\n",
    "\n",
    "        #Transcribe\n",
    "        self.transcription = self.transcribe()\n",
    "\n",
    "    def save_audio(self):\n",
    "        self.file_id = len(os.listdir('/content/sounds/'))\n",
    "        with open(f\"/content/sounds/sound-{self.file_id}.wav\", \"wb\") as audio_file:\n",
    "            audio_file.seek(0)\n",
    "            audio_file.write(self.audio_bytes)\n",
    "\n",
    "\n",
    "    def convert_wav_to_16bit_mono(self):\n",
    "        try:\n",
    "            file_path = f\"/content/sounds/sound-{self.file_id}.wav\"\n",
    "            pac.convert_wav_to_16bit_mono(file_path,file_path)\n",
    "            return True\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "\n",
    "    def transcribe(self):\n",
    "        try:\n",
    "            file_path = f\"/content/sounds/sound-{self.file_id}.wav\"\n",
    "            result= hf_model.transcribe([file_path])\n",
    "            return result[0]\n",
    "        except FileNotFoundError:\n",
    "            return \"Unable to transcribe audio!\"\n"
   ],
   "metadata": {
    "id": "xw8dDmjyUDu1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# -------------------- Text-to-Speech Conversation Logic --------------------\n",
    "\n",
    "def get_text(text, hps):\n",
    "    \"\"\"\n",
    "    Convert input text to a sequence of phonetic IDs for TTS processing.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text in Kinyarwanda\n",
    "        hps: Model hyperparameters\n",
    "        \n",
    "    Returns:\n",
    "        torch.LongTensor: Tensor of token IDs representing the input text\n",
    "    \"\"\"\n",
    "    # Convert text to sequence of phonetic IDs\n",
    "    text_norm = text_to_sequence(text)\n",
    "    \n",
    "    # Add blank tokens between phonemes if required by the model\n",
    "    if hps.data.add_blank:\n",
    "        text_norm = intersperse(text_norm, 0)\n",
    "    \n",
    "    # Convert to PyTorch tensor\n",
    "    text_norm = torch.LongTensor(text_norm)\n",
    "    return text_norm\n",
    "\n",
    "def kinya_tts(inputstr, output_folder='/content/outputs') -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Generate Kinyarwanda speech from text input.\n",
    "    \n",
    "    This function:\n",
    "    1. Processes the input text to remove special characters\n",
    "    2. Converts text to phonetic sequence\n",
    "    3. Runs inference using the TTS model\n",
    "    4. Adjusts volume and saves the output as a WAV file\n",
    "    \n",
    "    Args:\n",
    "        inputstr (str): Input text in Kinyarwanda\n",
    "        output_folder (str): Directory to save the generated audio\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[str, float]: Path to the generated audio file and its duration in seconds\n",
    "    \"\"\"\n",
    "    global inference_engine\n",
    "    (device, tts_model, tts_hps, louder_vol) = inference_engine\n",
    "    \n",
    "    # Remove brackets and other special characters that might affect pronunciation\n",
    "    fltstr = re.sub(r\"[\\[\\](){}]\", \"\", inputstr)\n",
    "    \n",
    "    # Convert text to phonetic sequence\n",
    "    stn_tst = get_text(fltstr, tts_hps)\n",
    "    \n",
    "    # Set speech generation speed (slightly slower than normal for clarity)\n",
    "    speed = 0.97\n",
    "    \n",
    "    # Generate audio using the TTS model\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        # Prepare input tensors\n",
    "        x_tst = stn_tst.to(device).unsqueeze(0)  # Add batch dimension\n",
    "        x_tst_lengths = torch.LongTensor([stn_tst.size(0)]).to(device)\n",
    "        \n",
    "        # Run inference with specific parameters\n",
    "        # - noise_scale: Controls speaking variation (0.667 is moderate)\n",
    "        # - noise_scale_w: Controls prosody variation (0.8 is natural)\n",
    "        # - length_scale: Controls speaking speed (1/speed adjusts pace)\n",
    "        audio = tts_model.infer(\n",
    "            x_tst, \n",
    "            x_tst_lengths, \n",
    "            noise_scale=.667, \n",
    "            noise_scale_w=0.8, \n",
    "            length_scale=1/speed\n",
    "        )[0][0, 0].data.cpu().float()\n",
    "    \n",
    "    # Calculate audio duration in seconds\n",
    "    AUDIO_TIME = audio.size(0) / tts_hps.data.sampling_rate\n",
    "    \n",
    "    # Increase volume for better audibility\n",
    "    audio = louder_vol(audio.unsqueeze(0))\n",
    "    \n",
    "    # Generate unique filename using UUID to prevent overwriting\n",
    "    unique_id = uuid.uuid4().hex\n",
    "    output_wav_file = f\"{output_folder}/output_{unique_id}.wav\"\n",
    "    \n",
    "    # Save audio to WAV file\n",
    "    torchaudio.save(output_wav_file, audio, tts_hps.data.sampling_rate)\n",
    "    \n",
    "    return output_wav_file, AUDIO_TIME"
   ],
   "metadata": {
    "id": "-rxM2gjUcTfT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "### NLP logic to return and answer questions.\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Define intents, patterns, and responses separately\n",
    "INTENTS = {\n",
    "    \"greeting\": {\n",
    "        \"patterns\": [\"muraho\", \"hello\", \"salut\", \"mwiriwe\", \"wiriweho\"],\n",
    "        \"responses\": [\n",
    "            \"Muraho neza! Nishimiye kukubona hano. Ufite ikibazo ushaka kumbaza?\",\n",
    "            \"Mwiriwe neza! Umeze neza?\",\n",
    "            \"Wiriweho neza! Nishimiye kuganira nawe.\"\n",
    "        ]\n",
    "    },\n",
    "    \"ask_owner\": {\n",
    "        \"patterns\": [\"uri nde?\", \"wakozwe nande?\", \"uturuka he?\"],\n",
    "        \"responses\": [\n",
    "            \"Ndi gacurabwenge umuhanga w'abanyarwanda\"\n",
    "        ]\n",
    "    },\n",
    "    \"ask_news\": {\n",
    "        \"patterns\": [\"amakuru\", \"amakuru yawe\", \"amakuru y'umunsi\", \"amakuru y'icyumweru\", \"amakuru ya mugitondo\", \"amakuru ya nijoro\"],\n",
    "        \"responses\": [\n",
    "            \"Amakuru ni meza cyane, urakoze kubaza!\",\n",
    "            \"Umunsi wagenze neza, ndashimira Imana. Wowe se?\",\n",
    "            \"Icyumweru cyagenze neza rwose, wowe ho?\",\n",
    "            \"Mugitondo wagenze neza cyane! Wowe uko byagenze?\",\n",
    "            \"Nijoro wagenze neza cyane, ndabashimiye!\",\n",
    "        ]\n",
    "    },\n",
    "    \"gratitude\": {\n",
    "        \"patterns\": [\"urakoze\", \"murakoze\", \"murakoze cyane\", \"ndashimira\", \"turabashimira\"],\n",
    "        \"responses\": [\n",
    "            \"Turagushimiye cyane nawe!\",\n",
    "            \"Murakoze cyane! Imana ibahe umugisha.\",\n",
    "            \"Twishimiye ko dukorana neza. Urakoze cyane!\"\n",
    "        ]\n",
    "    },\n",
    "    \"help_request\": {\n",
    "        \"patterns\": [\"wamfasha\", \"mfite ikibazo\", \"ndifuza ubufasha\", \"ushobora kungira inama\"],\n",
    "        \"responses\": [\n",
    "            \"Yego rwose, mbwira icyo ukeneye!\",\n",
    "            \"Nditeguye kugufasha. Mbwira ikibazo cyawe.\",\n",
    "            \"Nta kibazo, reka tuganire. Ukeneye iki?\"\n",
    "        ]\n",
    "    },\n",
    "    \"apology\": {\n",
    "        \"patterns\": [\"mbabarira\", \"ndababarira\", \"ndasaba imbabazi\"],\n",
    "        \"responses\": [\n",
    "            \"Birumvikana! Twese tugira ibyo dukosamo.\",\n",
    "            \"Imbabazi zawe zirakiriwe. Duhitemo gutera imbere.\",\n",
    "            \"Ntacyo bibaye, humura.\"\n",
    "        ]\n",
    "    },\n",
    "    \"farewell\": {\n",
    "        \"patterns\": [\"murabeho\", \"tuzabonana\", \"imana iguhe umugisha\"],\n",
    "        \"responses\": [\n",
    "            \"Murabeho neza! Imana iguhe imigisha myinshi.\",\n",
    "            \"Tuzabonana ubutaha, Imana ikomeze ikurinde!\",\n",
    "            \"Murabeho! Wihangane kandi ukomeze utsinde.\"\n",
    "        ]\n",
    "    },\n",
    "    \"default\": {\n",
    "        \"patterns\": [],\n",
    "        \"responses\": [\n",
    "            \"Mbabarira, sinabyumvise neza. Wansobanurira neza icyo ushaka?\",\n",
    "            \"Ndagusabye kongera usobanura neza.\",\n",
    "            \"Ndashaka kukumva neza, mbwira witonze.\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Additional direct questions and answers\n",
    "QA = {\n",
    "    \"Rwanda Coding Academy iherereye he?\": \"Iherereye mu Karere ka Nyabihu, mu Ntara y'Iburengerazuba.\",\n",
    "    \"Umurwa mukuru w'u Rwanda ni uwuhe?\": \"Ni Kigali.\",\n",
    "    \"Ni inde Perezida w'u Rwanda?\": \"Ni Paul Kagame.\",\n",
    "    \"Ikirere cy'uyu munsi kimeze gite?\": \"Sinabasha kukubwira ikirere nyacyo, ariko ushobora kugenzura kuri meteo.gov.rw.\",\n",
    "    \"Uburebure bwa Mount Kigali ni bungana iki?\": \"Bufite uburebure bwa metero zirenga 1,800.\",\n",
    "    \"U Rwanda rufite intara zingana iki?\": \"Rufite intara 5: Intara y'Amajyaruguru, Amajyepfo, Iburengerazuba, Iburasirazuba, n'Umujyi wa Kigali.\"\n",
    "}\n",
    "\n",
    "# Create keyword mappings for QA\n",
    "KEYWORD_MAP = {}\n",
    "for question, answer in QA.items():\n",
    "    # Extract words from question\n",
    "    normalized_question = normalize_text(question)\n",
    "    words = normalized_question.split()\n",
    "\n",
    "    # Create combinations of 2 and 3 keywords\n",
    "    from itertools import combinations\n",
    "    for n in [2, 3]:\n",
    "        if len(words) >= n:\n",
    "            keyword_combos = combinations(words, n)\n",
    "            for combo in keyword_combos:\n",
    "                key = \" \".join(sorted(combo))  # Sort to ensure consistent key\n",
    "                if key not in KEYWORD_MAP:\n",
    "                    KEYWORD_MAP[key] = []\n",
    "                KEYWORD_MAP[key].append(question)\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Remove punctuation, lowercase, and strip whitespace\n",
    "    text = text.strip().lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "def match_intent(normalized_input):\n",
    "    for intent, data in INTENTS.items():\n",
    "        for pattern in data[\"patterns\"]:\n",
    "            normalized_pattern = normalize_text(pattern)\n",
    "            if normalized_pattern in normalized_input:\n",
    "                return intent\n",
    "    return \"default\"\n",
    "\n",
    "def match_with_keywords(normalized_input):\n",
    "    words = normalized_input.split()\n",
    "\n",
    "    # Try to match with exact question first\n",
    "    for question in QA:\n",
    "        if normalize_text(question) == normalized_input:\n",
    "            return question\n",
    "\n",
    "    # Try combinations of keywords in the input\n",
    "    potential_matches = {}\n",
    "\n",
    "    # Check combinations of 3 keywords first (more specific)\n",
    "    if len(words) >= 3:\n",
    "        for combo in combinations(words, 3):\n",
    "            key = \" \".join(sorted(combo))\n",
    "            if key in KEYWORD_MAP:\n",
    "                for question in KEYWORD_MAP[key]:\n",
    "                    potential_matches[question] = potential_matches.get(question, 0) + 3  # Higher weight for 3-word match\n",
    "\n",
    "    # Then check combinations of 2 keywords\n",
    "    if len(words) >= 2:\n",
    "        for combo in combinations(words, 2):\n",
    "            key = \" \".join(sorted(combo))\n",
    "            if key in KEYWORD_MAP:\n",
    "                for question in KEYWORD_MAP[key]:\n",
    "                    potential_matches[question] = potential_matches.get(question, 0) + 1\n",
    "\n",
    "    # Return the question with highest match score if any\n",
    "    if potential_matches:\n",
    "        best_match = max(potential_matches.items(), key=lambda x: x[1])[0]\n",
    "        return best_match\n",
    "\n",
    "    return None\n",
    "\n",
    "def get_response(user_input):\n",
    "    normalized_input = normalize_text(user_input)\n",
    "\n",
    "    # First try exact match or keyword-based matching\n",
    "    matched_question = match_with_keywords(normalized_input)\n",
    "    if matched_question:\n",
    "        return QA[matched_question]\n",
    "\n",
    "    # Fallback to intent-based response\n",
    "    intent = match_intent(normalized_input)\n",
    "    response = random.choice(INTENTS[intent][\"responses\"])\n",
    "    return response"
   ],
   "metadata": {
    "id": "gplCwP0dcvOL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def full_conversation(audio_file):\n",
    "    with open(audio_file, \"rb\") as f:\n",
    "        if not os.path.isfile(audio_file):\n",
    "          return \"No audio detected, please try again.\"\n",
    "        audio_bytes = f.read()\n",
    "\n",
    "    # Step 1: Transcribe the audio\n",
    "    transcriber = Transcriber(audio_bytes)\n",
    "    user_text = transcriber.transcription.text\n",
    "    print(f\"User: {user_text}\")\n",
    "\n",
    "    # Step 2: Get the chatbot response\n",
    "    response_text = get_response(user_text)\n",
    "    print(f\"Assistant Response: {response_text}\")\n",
    "\n",
    "    # Step 3: Convert chatbot response to speech\n",
    "    wav_file, _ = kinya_tts(response_text)\n",
    "\n",
    "    # Step 4: Return transcription and audio response\n",
    "    return user_text, wav_file"
   ],
   "metadata": {
    "id": "s94BH0yEcb21"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "kinya_tts_setup()  # Initialize TTS engine once at startup"
   ],
   "metadata": {
    "id": "0zXlewGScirJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## Gradio Interface for Launching the TTS\n",
    "conversation_demo = gr.Interface(\n",
    "    fn=full_conversation,\n",
    "    inputs=gr.Audio(type=\"filepath\", label=\"Upload Kinyarwanda Audio\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Transcribed User Text\"),\n",
    "        gr.Audio(label=\"Assistant Voice Response\", autoplay=True)\n",
    "    ],\n",
    "    title=\"Kinyarwanda Voice Assistant\",\n",
    "    description=\"Speak in Kinyarwanda, get a transcription + voice reply generated by AI!\",\n",
    "    examples=[\n",
    "        [\"/content/sounds/sound-0.wav\"],\n",
    "        [\"/content/sounds/sound-1.wav\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Launch\n",
    "conversation_demo.launch(debug=True)"
   ],
   "metadata": {
    "id": "md5mhCMQcpMP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# AI as the MiddleWare Section(Used GPT API-key to answer) questions."
   ],
   "metadata": {
    "id": "iR7Qx4xUxwf6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "## First Installing openai\n",
    "!pip install --quiet openai"
   ],
   "metadata": {
    "id": "OgoO-Tr_x52u",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9d5d02d8-f773-4a7b-ddcc-68a1f69816af"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[33mWARNING: Ignoring invalid distribution ~rotobuf (/usr/local/lib/python3.11/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution ~rotobuf (/usr/local/lib/python3.11/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution ~rotobuf (/usr/local/lib/python3.11/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution ~rotobuf (/usr/local/lib/python3.11/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ZH8pJSLqe8Gh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## ChatGPT Function Key\n",
    "from google.colab import userdata\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=userdata.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def call_chatgpt(prompt: str) -> str:\n",
    "    \"\"\"Call GPT as a fallback and return its reply.\"\"\"\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=150,\n",
    "            temperature=0.3,\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        # Log the error and return a safe fallback\n",
    "        print(f\"[ChatGPT API error] {e}\")\n",
    "        return (\"Simbashije kubona igisubizo ongera gerageza mukanya\")"
   ],
   "metadata": {
    "id": "_w0QJG27hKPa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "### NLP Logic manual with also CHATGPT enabled.\n",
    "import os\n",
    "import openai\n",
    "import random\n",
    "import string\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "# Define intents, patterns, and responses separately\n",
    "INTENTS = {\n",
    "    \"greeting\": {\n",
    "        \"patterns\": [\"muraho\", \"hello\", \"salut\", \"mwiriwe\", \"wiriweho\"],\n",
    "        \"responses\": [\n",
    "            \"Muraho neza! Nishimiye kukubona hano. Ufite ikibazo ushaka kumbaza?\",\n",
    "            \"Mwiriwe neza! Umeze neza?\",\n",
    "            \"Wiriweho neza! Nishimiye kuganira nawe.\"\n",
    "        ]\n",
    "    },\n",
    "    \"ask_owner\": {\n",
    "        \"patterns\": [\"uri nde?\", \"wakozwe nande?\", \"uturuka he?\"],\n",
    "        \"responses\": [\n",
    "            \"Ndi gacurabwenge umuhanga w'abanyarwanda\"\n",
    "        ]\n",
    "    },\n",
    "    \"ask_news\": {\n",
    "        \"patterns\": [\"amakuru\", \"amakuru yawe\", \"amakuru y'umunsi\", \"amakuru y'icyumweru\", \"amakuru ya mugitondo\", \"amakuru ya nijoro\"],\n",
    "        \"responses\": [\n",
    "            \"Amakuru ni meza cyane, urakoze kubaza!\",\n",
    "            \"Umunsi wagenze neza, ndashimira Imana. Wowe se?\",\n",
    "            \"Icyumweru cyagenze neza rwose, wowe ho?\",\n",
    "            \"Mugitondo wagenze neza cyane! Wowe uko byagenze?\",\n",
    "            \"Nijoro wagenze neza cyane, ndabashimiye!\",\n",
    "        ]\n",
    "    },\n",
    "    \"gratitude\": {\n",
    "        \"patterns\": [\"urakoze\", \"murakoze\", \"murakoze cyane\", \"ndashimira\", \"turabashimira\"],\n",
    "        \"responses\": [\n",
    "            \"Turagushimiye cyane nawe!\",\n",
    "            \"Murakoze cyane! Imana ibahe umugisha.\",\n",
    "            \"Twishimiye ko dukorana neza. Urakoze cyane!\"\n",
    "        ]\n",
    "    },\n",
    "    \"help_request\": {\n",
    "        \"patterns\": [\"wamfasha\", \"mfite ikibazo\", \"ndifuza ubufasha\", \"ushobora kungira inama\"],\n",
    "        \"responses\": [\n",
    "            \"Yego rwose, mbwira icyo ukeneye!\",\n",
    "            \"Nditeguye kugufasha. Mbwira ikibazo cyawe.\",\n",
    "            \"Nta kibazo, reka tuganire. Ukeneye iki?\"\n",
    "        ]\n",
    "    },\n",
    "    \"apology\": {\n",
    "        \"patterns\": [\"mbabarira\", \"ndababarira\", \"ndasaba imbabazi\"],\n",
    "        \"responses\": [\n",
    "            \"Birumvikana! Twese tugira ibyo dukosamo.\",\n",
    "            \"Imbabazi zawe zirakiriwe. Duhitemo gutera imbere.\",\n",
    "            \"Ntacyo bibaye, humura.\"\n",
    "        ]\n",
    "    },\n",
    "    \"farewell\": {\n",
    "        \"patterns\": [\"murabeho\", \"tuzabonana\", \"imana iguhe umugisha\"],\n",
    "        \"responses\": [\n",
    "            \"Murabeho neza! Imana iguhe imigisha myinshi.\",\n",
    "            \"Tuzabonana ubutaha, Imana ikomeze ikurinde!\",\n",
    "            \"Murabeho! Wihangane kandi ukomeze utsinde.\"\n",
    "        ]\n",
    "    },\n",
    "    \"default\": {\n",
    "        \"patterns\": [],\n",
    "        \"responses\": [\n",
    "            \"Mbabarira, sinabyumvise neza. Wansobanurira neza icyo ushaka?\",\n",
    "            \"Ndagusabye kongera usobanura neza.\",\n",
    "            \"Ndashaka kukumva neza, mbwira witonze.\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Additional direct questions and answers\n",
    "QA = {\n",
    "    \"Rwanda Coding Academy iherereye he?\": \"Iherereye mu Karere ka Nyabihu, mu Ntara y'Iburengerazuba.\",\n",
    "    \"Umurwa mukuru w'u Rwanda ni uwuhe?\": \"Ni Kigali.\",\n",
    "    \"Ni inde Perezida w'u Rwanda?\": \"Ni Paul Kagame.\",\n",
    "    \"Ikirere cy'uyu munsi kimeze gite?\": \"Sinabasha kukubwira ikirere nyacyo, ariko ushobora kugenzura kuri meteo.gov.rw.\",\n",
    "    \"Uburebure bwa Mount Kigali ni bungana iki?\": \"Bufite uburebure bwa metero zirenga 1,800.\",\n",
    "    \"U Rwanda rufite intara zingana iki?\": \"Rufite intara 5: Intara y'Amajyaruguru, Amajyepfo, Iburengerazuba, Iburasirazuba, n'Umujyi wa Kigali.\"\n",
    "}\n",
    "\n",
    "# Create keyword mappings for QA\n",
    "KEYWORD_MAP = {}\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Remove punctuation, lowercase, and strip whitespace\n",
    "    text = text.strip().lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "def match_intent(normalized_input):\n",
    "    for intent, data in INTENTS.items():\n",
    "        for pattern in data[\"patterns\"]:\n",
    "            normalized_pattern = normalize_text(pattern)\n",
    "            if normalized_pattern in normalized_input:\n",
    "                return intent\n",
    "    return \"default\"\n",
    "\n",
    "def get_response(user_input: str) -> str:\n",
    "    normalized = normalize_text(user_input)\n",
    "\n",
    "    # 2. Try Intent Based.\n",
    "    intent = match_intent(normalized)\n",
    "    if intent != \"default\":\n",
    "        return random.choice(INTENTS[intent][\"responses\"])\n",
    "\n",
    "    # 3. Fallback: delegate to ChatGPT\n",
    "    return call_chatgpt(user_input)"
   ],
   "metadata": {
    "id": "jTtKIKW_fgyc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "p4gkY_Qjfy3z"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def full_conversation(audio_file):\n",
    "    with open(audio_file, \"rb\") as f:\n",
    "        if not os.path.isfile(audio_file):\n",
    "          return \"No audio detected, please try again.\"\n",
    "        audio_bytes = f.read()\n",
    "\n",
    "    # Step 1: Transcribe the audio\n",
    "    transcriber = Transcriber(audio_bytes)\n",
    "    user_text = transcriber.transcription.text\n",
    "    print(f\"User: {user_text}\")\n",
    "\n",
    "    # Step 2: Get the chatbot response\n",
    "    response_text = get_response(user_text)\n",
    "    print(f\"Assistant Response: {response_text}\")\n",
    "\n",
    "    # Step 3: Convert chatbot response to speech\n",
    "    wav_file, _ = kinya_tts(response_text)\n",
    "\n",
    "    # Step 4: Return transcription and audio response\n",
    "    return user_text, wav_file"
   ],
   "metadata": {
    "id": "ofzB4rQugObv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "kinya_tts_setup()  # Initialize TTS engine once at startup"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cK247B2AgcmZ",
    "outputId": "d27a95f1-b7da-42bb-968c-dbd3c46c265c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using mel posterior encoder for VITS2\n",
      "Multi-stream iSTFT VITS2\n",
      "TTS API engine ready!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "## Gradio Interface for Launching the TTS\n",
    "conversation_demo = gr.Interface(\n",
    "    fn=full_conversation,\n",
    "    inputs=gr.Audio(type=\"filepath\", label=\"Upload Kinyarwanda Audio\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Transcribed User Text\"),\n",
    "        gr.Audio(label=\"Assistant Voice Response\", autoplay=True)\n",
    "    ],\n",
    "    title=\"Kinyarwanda Voice Assistant\",\n",
    "    description=\"Speak in Kinyarwanda, get a transcription + voice reply generated by AI!\",\n",
    "    examples=[\n",
    "        [\"/content/sounds/sound-0.wav\"],\n",
    "        [\"/content/sounds/sound-1.wav\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Launch\n",
    "conversation_demo.launch(debug=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "YM08VxV-gZnh",
    "outputId": "27674307-026a-4ddc-caff-e8c98f126d31"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "* Running on public URL: https://c1f65cef94afa1c877.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"https://c1f65cef94afa1c877.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Transcribing:   0%|          | 0/1 [00:00<?, ?it/s][NeMo W 2025-04-28 05:10:03 nemo_logging:405] CTC decoding strategy 'greedy' is slower than 'greedy_batch', which implements the same exact interface. Consider changing your strategy to 'greedy_batch' for a free performance improvement.\n",
      "Transcribing: 100%|██████████| 1/1 [00:00<00:00,  2.00it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "User: afurika ifite ibihugu bingahe\n",
      "Assistant Response: Afurika ifite ibihugu 54 byigenga. Ibi bihugu bigize umugabane wa kabiri mu bunini no mu bwinshi bw'abaturage ku isi.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Transcribing: 100%|██████████| 1/1 [00:00<00:00, 21.65it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "User: umwami wambere wu rwanda nuwube\n",
      "Assistant Response: Umwami wa mbere wa Rwanda yari Yuhi I Gahima, umwe mu bami ba kera b'ibihugu by'Abanyarwanda. Hariho urutonde rw'abami b'u Rwanda, kandi Yuhi I Gahima ni umwe mu bami ba mbere bazwi mu mateka y'u Rwanda. Nyuma ye, hari hagiyeho abandi bami benshi kugeza ku mwami wa nyuma, Kigeli V Ndahindurwa, wahagaritse ingoma mu 1961 ubwo u Rwanda rwahindukaga repubulika.\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://c1f65cef94afa1c877.gradio.live\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "execution_count": 10
    }
   ]
  }
 ]
}
